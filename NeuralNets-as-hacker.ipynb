{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://karpathy.github.io/neuralnets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## circuits with one gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardMultiplyGate(x,y):\n",
    "    return x * y\n",
    "\n",
    "x = -2\n",
    "y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tweak = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_out = - np.inf\n",
    "best_x = x \n",
    "best_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random local search\n",
    "\n",
    "for i in xrange(100):\n",
    "    x_try = x + tweak * (np.random.normal()*2 - 1)\n",
    "    y_try = y + tweak * (np.random.normal()*2 - 1)\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    if out > best_out:\n",
    "        best_out = out\n",
    "        best_x = x_try\n",
    "        best_y = y_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.99351370814 2.95278236817 -5.88641212812\n"
     ]
    }
   ],
   "source": [
    "print best_x,best_y,best_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.00000000000189"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerical gradient\n",
    "# intuition: force x to be larger, force y to be lower, to make output larger than -6\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x,y)\n",
    "h = 0.0001 # small enough, works fine in most cases\n",
    "\n",
    "xph = x + h\n",
    "out2 = forwardMultiplyGate(xph,y)\n",
    "x_derivative = (out2 - out)/h\n",
    "x_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0000000000042206"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yph = y + h\n",
    "out3 = forwardMultiplyGate(x, yph)\n",
    "y_derivative = (out3 - out)/h \n",
    "y_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.87059999999986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01 # bigger steps not always better\n",
    "out = forwardMultiplyGate(x,y)\n",
    "x = x + step_size * x_derivative\n",
    "y = y + step_size * y_derivative\n",
    "out_new = forwardMultiplyGate(x,y)\n",
    "out_new # output increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analytic gradient\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x,y)\n",
    "x_gradient = y # f(x,y)=xy, x's gradient is y\n",
    "y_gradient = x # y's gradient is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.8706"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x += step_size * x_gradient\n",
    "y += step_size * y_gradient\n",
    "out_new = forwardMultiplyGate(x,y)\n",
    "out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际运用中, 都会计算 analytic gradient, 但用numerical gradient 来验证. 因为 numerical 的方法虽然简单但计算昂贵, analytic 虽然有 bugs 但会很高效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## circuits with multiple gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(a,b):\n",
    "    return a * b\n",
    "def forwardAddGate(a,b):\n",
    "    return a + b\n",
    "\n",
    "def forwardCircuit(x,y,z):\n",
    "    q = forwardAddGate(x,y) # q = x + y\n",
    "    f = forwardMultiplyGate(q,z) # f = q*z\n",
    "    return f\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "forwardCircuit(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backpropagation\n",
    "Chain rule\n",
    "\n",
    "$\\frac{\\partial f(q,z)}{\\partial x} = \\frac{\\partial q(x,y)}{\\partial x} \\frac{\\partial f(q,z)}{\\partial q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "q = forwardAddGate(x,y)\n",
    "f = forwardMultiplyGate(q,z)\n",
    "\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "derivative_q_wrt_x = 1.0\n",
    "derivative_q_wrt_y = 1.0\n",
    "\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_f_wrt_xyz = [derivative_f_wrt_x, derivative_f_wrt_y, derivative_f_wrt_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.92"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x = x + step_size * derivative_f_wrt_x\n",
    "y = y + step_size * derivative_f_wrt_y\n",
    "z = z + step_size * derivative_f_wrt_z\n",
    "\n",
    "q = forwardAddGate(x,y)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.5924"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = forwardMultiplyGate(q,z)\n",
    "f # up from -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.99999999999 -3.99999999999 3.00000000001\n"
     ]
    }
   ],
   "source": [
    "# numerical gradient check\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "\n",
    "h = 0.0001\n",
    "x_derivative = (forwardCircuit(x+h,y,z) - forwardCircuit(x,y,z)) / h\n",
    "y_derivative = (forwardCircuit(x,y+h,z) - forwardCircuit(x,y,z)) / h\n",
    "z_derivative = (forwardCircuit(x,y,z+h) - forwardCircuit(x,y,z)) / h\n",
    "print x_derivative, y_derivative, z_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: single neuron\n",
    " f(x,y,a,b,c)= σ(ax+by+c)\n",
    "\n",
    "sigmoid function - output 0 to 1\n",
    "\n",
    "$\\frac{\\partial \\sigma(x)}{\\partial x}= \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(this, value,grad):\n",
    "        this.value = value\n",
    "        this.grad = grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class multiplyGate:\n",
    "    def forward(this, u0, u1):\n",
    "        # store pointers to input Units u0 and u1 and output unit utop\n",
    "        this.u0 = u0\n",
    "        this.u1 = u1\n",
    "        this.utop = Unit(u0.value*u1.value, 0.0)\n",
    "        return this.utop\n",
    "    \n",
    "    def backward(this):\n",
    "        # take the gradient in output unit and chain it with the\n",
    "        # local gradients, which we derived for multiply gate before\n",
    "        # then write those gradients to those Units.\n",
    "        this.u0.grad += this.u1.value * this.utop.grad\n",
    "        this.u1.grad += this.u0.value * this.utop.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class addGate:\n",
    "    def forward(this, u0, u1):\n",
    "        this.u0 = u0\n",
    "        this.u1 = u1\n",
    "        this.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return this.utop\n",
    "    def backward(this):\n",
    "        this.u0.grad += 1 * this.utop.grad\n",
    "        this.u1.grad += 1 * this.utop.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class sigmoidGate:\n",
    "    \n",
    "    def forward(this, u0):\n",
    "        this.u0 = u0\n",
    "        this.utop = Unit(sig(this.u0.value), 0.0)\n",
    "        return this.utop\n",
    "    def backward(this):\n",
    "        s = sig(this.u0.value)\n",
    "        this.u0.grad += (s*(1-s))*this.utop.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "a = Unit(1.0,0.0)\n",
    "b = Unit(2.0,0.0)\n",
    "c = Unit(-3.0,0.0)\n",
    "x = Unit(-1.0,0.0)\n",
    "y = Unit(3.0,0.0)\n",
    "\n",
    "# gates\n",
    "mulg0 = multiplyGate()\n",
    "mulg1 = multiplyGate()\n",
    "addg0 = addGate()\n",
    "addg1 = addGate()\n",
    "sg0 = sigmoidGate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880797077978\n"
     ]
    }
   ],
   "source": [
    "# forward pass forwardNeuron\n",
    "def forwardNeuron():\n",
    "    ax = mulg0.forward(a,x)\n",
    "    by = mulg1.forward(b,y)\n",
    "    axpby = addg0.forward(ax,by)\n",
    "    axpbypc = addg1.forward(axpby,c)\n",
    "    s = sg0.forward(axpbypc)\n",
    "    return s\n",
    "s = forwardNeuron()\n",
    "print s.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backward \n",
    "s.grad = 1.0 # set to be 1, if not, all gradient wil be 0\n",
    "sg0.backward()   # writes gradient into axpbypc\n",
    "addg1.backward() # writes gradients into axpby and c\n",
    "addg0.backward() # writes gradients into ax and by\n",
    "mulg1.backward() # writes gradients into b and y\n",
    "mulg0.backward() # writes gradients into a and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882550181622\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "\n",
    "a.value += step_size * a.grad\n",
    "b.value += step_size * b.grad\n",
    "c.value += step_size * c.grad\n",
    "x.value += step_size * x.grad\n",
    "y.value += step_size * y.grad\n",
    "\n",
    "s = forwardNeuron()\n",
    "print s.value # 0.88255 is higher than 0.88080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verify implemented backpropagation\n",
    "\n",
    "def forwardCircuitFast(a,b,c,x,y):\n",
    "    return 1/(1 + np.exp( - (a*x + b*y + c)))\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "c = -3\n",
    "x = -1\n",
    "y = 3\n",
    "\n",
    "h = 0.0001\n",
    "a_grad = (forwardCircuitFast(a+h,b,c,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "b_grad = (forwardCircuitFast(a,b+h,c,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "c_grad = (forwardCircuitFast(a,b,c+h,x,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "x_grad = (forwardCircuitFast(a,b,c,x+h,y) - forwardCircuitFast(a,b,c,x,y))/h\n",
    "y_grad = (forwardCircuitFast(a,b,c,x,y+h) - forwardCircuitFast(a,b,c,x,y))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10499758359205913, 0.31494477483517969, 0.10498958734506125, 0.10498958734506125, 0.2099711788272618]\n"
     ]
    }
   ],
   "source": [
    "print [a_grad,b_grad,c_grad,x_grad,y_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All you have to do is write small gates that compute local, simple derivatives w.r.t their inputs, wire it up in a graph, do a forward pass to compute the output value and then a backward pass that chains the gradients all the way to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiply gate - swither\n",
    "x = a*b\n",
    "da = b*dx\n",
    "db = a*dx\n",
    "\n",
    "# add gate\n",
    "x = a + b\n",
    "da = 1 * dx\n",
    "db = 1 * dx\n",
    "\n",
    "# x = a + b + c compute in two steps\n",
    "q = a + b\n",
    "x = q + c\n",
    "# backward pass\n",
    "dc = 1 * dx\n",
    "dq = 1 * dx\n",
    "da = 1 * dq\n",
    "db = 1 * dq\n",
    "# -->\n",
    "x = a + b + c\n",
    "da = 1 * dx\n",
    "db = 1 * dx\n",
    "dc = 1 * dx\n",
    "\n",
    "# combining gates\n",
    "x = a * b + c\n",
    "da = b * dx\n",
    "db = a * dx\n",
    "dc = 1 * dx \n",
    "\n",
    "# neuron case\n",
    "q = a*x + b*y + c\n",
    "f = sig(q)\n",
    "df = 1\n",
    "dq = (f*(1-f))*df\n",
    "da = x * dq\n",
    "dx = a * dq\n",
    "dy = b * dq\n",
    "db = y * dq\n",
    "dc = 1 * dq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chapter 1 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary classification\n",
    "'''\n",
    "vector -> label\n",
    "---------------\n",
    "[1.2, 0.7] -> +1\n",
    "[-0.3, 0.5] -> -1\n",
    "[-3, -1] -> +1\n",
    "[0.1, 1.0] -> -1\n",
    "[3.0, 1.1] -> -1\n",
    "[2.1, -3] -> +1\n",
    "'''\n",
    "\n",
    "# learn support vector machine\n",
    "\n",
    "#A circuit: it takes 5 Units (x,y,a,b,c) and outputs a single Unit\n",
    "# It can also compute the gradient w.r.t. its inputs\n",
    "class Circuit:\n",
    "    def __init__(this): # gates\n",
    "        this.mulg0 = multiplyGate()\n",
    "        this.mulg1 = multiplyGate()\n",
    "        this.addg0 = addGate()\n",
    "        this.addg1 = addGate()\n",
    "    def forward(this,x,y,a,b,c):\n",
    "        this.ax = this.mulg0.forward(a,x)\n",
    "        this.by = this.mulg1.forward(b,y)\n",
    "        this.axpby = this.addg0.forward(this.ax, this.by)\n",
    "        this.axpbypc = this.addg1.forward(this.axpby, c)\n",
    "        return this.axpbypc\n",
    "    def backward(this,gradient_top):\n",
    "        this.axpbypc.grad = gradient_top\n",
    "        this.addg1.backward()\n",
    "        this.addg0.backward()\n",
    "        this.mulg1.backward()\n",
    "        this.mulg0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM class\n",
    "class SVM:\n",
    "    def __init__(this):\n",
    "        this.a = Unit(1.0,0.0)\n",
    "        this.b = Unit(-2.0,0.0)\n",
    "        this.c = Unit(-1.0,0.0)\n",
    "        this.circuit = Circuit()\n",
    "    def forward(this,x,y):\n",
    "        this.unit_out = this.circuit.forward(x,y, this.a, this.b, this.c)\n",
    "        return this.unit_out\n",
    "    def backward(this,label): # label is +1 or -1\n",
    "        this.a.grad = 0.0\n",
    "        this.b.grad = 0.0\n",
    "        this.c.grad = 0.0\n",
    "        pull = 0.0\n",
    "        if label==1 and this.unit_out.value<1:\n",
    "            pull = 1\n",
    "        if label==-1 and this.unit_out.value>-1:\n",
    "            pull = -1\n",
    "        this.circuit.backward(pull)\n",
    "        \n",
    "        this.a.grad += -this.a.value\n",
    "        this.b.grad += -this.b.value\n",
    "    def learnFrom(this,x,y,label):\n",
    "        this.forward(x,y)\n",
    "        this.backward(label)\n",
    "        this.parameterUpdate()\n",
    "    def parameterUpdate(this):\n",
    "        step_size = 0.01\n",
    "        this.a.value += step_size * this.a.grad\n",
    "        this.b.value += step_size * this.b.grad\n",
    "        this.c.value += step_size * this.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [[1.2,0.7],[-0.3,-0.5],[3.0,0.1],[-0.1,-1.0],[-1.0,1.1],[2.1,-3]]\n",
    "labels = [1,-1,1,-1,-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train SVM with SGD\n",
    "\n",
    "svm = SVM()\n",
    "\n",
    "#evaluate\n",
    "def evalTrainingAccuracy():\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        x = Unit(data[i][0],0.0)\n",
    "        y = Unit(data[i][1],0.0)\n",
    "        true_label = labels[i]\n",
    "        if svm.forward(x,y).value > 0:\n",
    "            predicted_label = 1\n",
    "        else:\n",
    "            predicted_label = -1\n",
    "        if predicted_label == true_label:\n",
    "            num_correct += 1\n",
    "    return float(num_correct)/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy at iteration 0 : 0.833333333333\n",
      "training accuracy at iteration 25 : 0.833333333333\n",
      "training accuracy at iteration 50 : 0.833333333333\n",
      "training accuracy at iteration 75 : 0.833333333333\n",
      "training accuracy at iteration 100 : 0.833333333333\n",
      "training accuracy at iteration 125 : 0.833333333333\n",
      "training accuracy at iteration 150 : 0.833333333333\n",
      "training accuracy at iteration 175 : 0.833333333333\n",
      "training accuracy at iteration 200 : 0.833333333333\n",
      "training accuracy at iteration 225 : 1.0\n",
      "training accuracy at iteration 250 : 0.833333333333\n",
      "training accuracy at iteration 275 : 0.833333333333\n",
      "training accuracy at iteration 300 : 0.833333333333\n",
      "training accuracy at iteration 325 : 1.0\n",
      "training accuracy at iteration 350 : 0.833333333333\n",
      "training accuracy at iteration 375 : 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# learning loop ???\n",
    "\n",
    "for iteration in range(400):\n",
    "    i = int(np.floor(np.random.uniform()*len(data)))\n",
    "    x = Unit(data[i][0],0.0)\n",
    "    y = Unit(data[i][1],0.0)\n",
    "    label = labels[i]\n",
    "    svm.learnFrom(x,y,label)\n",
    "    if iteration%25 == 0:\n",
    "        print \"training accuracy at iteration\",iteration,\":\",evalTrainingAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train SVM simpler way\n",
    "a = 1\n",
    "b = -2\n",
    "c = -1\n",
    "\n",
    "for iteration in range(400):\n",
    "    i = int(np.floor(np.random.uniform()*len(data)))\n",
    "    x = data[i][0]\n",
    "    y = data[i][1]\n",
    "    label = labels[i]\n",
    "    score = a*x+b*y+c\n",
    "    pull = 0.0\n",
    "    if label ==1 and score <1:\n",
    "        pull = 1\n",
    "    if label == -1 and score>-1:\n",
    "        pull = -1\n",
    "    step_size = 0.01\n",
    "    a += step_size*(x*pull-a)\n",
    "    b += step_size*(y*pull-b)\n",
    "    c += step_size*(1*pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generailizing SVM into Neural Nets\n",
    "# 2-layer neural nets does binary classification\n",
    "n1 = max(0, a1*x + b1*y + c1)\n",
    "n2 = max(0, a2*x + b2*y + c2)\n",
    "n3 = max(0, a3*x + b3*y + c3)\n",
    "score = a4*n1 + b4*n2 + c4*n3 + d4\n",
    "\n",
    "# 3 hidden neurons, n1,n2,n3, Rectified Linear Unit (ReLU) non-linearity on each hidden neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# every hidden neuron is a classifier, build extra classifier on top of them \n",
    "a1 = np.random.uniform()-0.5 #a random number between -0.5 and 0.5\n",
    "for iteration in range(400):\n",
    "    i = int(np.floor(np.random.uniform()*len(data)))\n",
    "    x = data[i][0]\n",
    "    y = data[i][1]\n",
    "    label = labels[i]\n",
    "    n1 = max(0, a1*x + b1*y + c1)\n",
    "    n2 = max(0, a2*x + b2*y + c2)\n",
    "    n3 = max(0, a3*x + b3*y + c3)\n",
    "    score = a4*n1 + b4*n2 + c4*n3 + d4\n",
    "    \n",
    "    pull = 0.0\n",
    "    if label ==1 and score <1:\n",
    "        pull = 1\n",
    "    if label == -1 and score>-1:\n",
    "        pull = -1\n",
    "    # backprop through the last \"score\" neuron     \n",
    "    dscore = pull\n",
    "    da4 = n1 * dscore\n",
    "    dna = a4 * dscore\n",
    "    db4 = n2 * dscore\n",
    "    dn2 = b4 * dscore\n",
    "    dc4 = n3 * dscore\n",
    "    dn3 = c4 * dscore\n",
    "    dd4 = 1.0 * dscore\n",
    "    # backprop the ReLU non-linearities, in place\n",
    "    # i.e. just set gradients to zero if the neurons did not \"fire\"\n",
    "    if n3 == 0:\n",
    "        dn3 = 0\n",
    "    if n2 == 0:\n",
    "        dn2 = 0\n",
    "    if n1 == 0:\n",
    "        dn1 = 0\n",
    "    \n",
    "    # backprop tp neuron 1\n",
    "    da1 = x * dn1\n",
    "    db1 = y * dn1\n",
    "    dc1 = 1.0 * dn1\n",
    "    \n",
    "    # backprop to neuron 2\n",
    "    da2 = x * dn2\n",
    "    db2 = y * dn2\n",
    "    dc2 = 1.0 * dn2\n",
    "    \n",
    "    # backprop to neuron 3\n",
    "    da3 = x * dn3\n",
    "    db3 = y * dn3\n",
    "    dc3 = 1.0 * dn3\n",
    "    \n",
    "    # add pulls from regularization\n",
    "    da1 += -a1; da2 += -a2; da3 += -a3;\n",
    "    db1 += -b1; db2 += -b2; db3 += -b3;\n",
    "    da4 += -a4; db4 += -b4; dc4 += -c4;\n",
    "    \n",
    "    # parameter update\n",
    "    step_size = 0.01\n",
    "    a1 += step_size * da1\n",
    "    b1 += step_size * db1 \n",
    "    c1 += step_size * dc1\n",
    "    a2 += step_size * da2 \n",
    "    b2 += step_size * db2\n",
    "    c2 += step_size * dc2\n",
    "    a3 += step_size * da3 \n",
    "    b3 += step_size * db3 \n",
    "    c3 += step_size * dc3\n",
    "    a4 += step_size * da4 \n",
    "    b4 += step_size * db4 \n",
    "    c4 += step_size * dc4 \n",
    "    d4 += step_size * dd4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function, 2-D SVM\n",
    "$L=[\\sum_{i=1}^{N}max(0,−y_i(w_0x_{i0}+w_1x_{i1}+w_2)+1)]+\\alpha[w_0^2+w_1^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [ [1.2, 0.7], [-0.3, 0.5], [3, 2.5] ]\n",
    "y = [1, -1, 1] # array of labels\n",
    "w = [0.1, 0.2, 0.3] # example: random weights\n",
    "alpha = 0.1 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(X,y,w):\n",
    "    total_cost = 0.0\n",
    "    N = len(X)\n",
    "    for i in range(N):\n",
    "        xi = X[i]\n",
    "        score = w[0]*xi[0]+w[1]*xi[1]+w[2]\n",
    "        \n",
    "        yi = y[i]\n",
    "        costi = max(0, -yi*score+1)\n",
    "        print 'example',i,':xi = (',xi,') and label =',yi\n",
    "        print '  score computed to be ',score\n",
    "        print '  => cost computed to be ',costi\n",
    "        total_cost += costi\n",
    "    \n",
    "    reg_cost = alpha*(w[0]*w[0]+w[1]*w[1])\n",
    "    print 'regularization cost for current model is ',reg_cost\n",
    "    total_cost += reg_cost\n",
    "    \n",
    "    print 'total cost is ',total_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0 :xi = ( [1.2, 0.7] ) and label = 1\n",
      "  score computed to be  0.56\n",
      "  => cost computed to be  0.44\n",
      "example 1 :xi = ( [-0.3, 0.5] ) and label = -1\n",
      "  score computed to be  0.37\n",
      "  => cost computed to be  1.37\n",
      "example 2 :xi = ( [3, 2.5] ) and label = 1\n",
      "  score computed to be  1.1\n",
      "  => cost computed to be  0\n",
      "regularization cost for current model is  0.005\n",
      "total cost is  1.815\n"
     ]
    }
   ],
   "source": [
    "total_cost = cost(X,y,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
