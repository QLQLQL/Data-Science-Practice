{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 15 # epoch to train, 其实这个是指总的循环次数吗?\n",
    "word_embed_dim = 650 # dimension of word embedding matrix 词向量矩阵\n",
    "char_embed_dim = 15  # dimension of char embedding matrix 字向量矩阵\n",
    "max_word_lenth = 65  # 词的最长长度, 这个对于中文没有必要好像...\n",
    "batch_size = 100     # batch 最小的批处理\n",
    "seq_length = 35      # no. of timesteps to unroll for ???\n",
    "learning_Rate = 1.0\n",
    "decay = 0.5  # decay of SGD\n",
    "dropout_prob = 0.5   # probability of dropout layer, dropout layer到底是用在哪儿的???\n",
    "feature_maps = [50,100,150,200,200,200,200] # no. of feature maps in CNN ???\n",
    "kernels = [1,2,3,4,5,6,7]  # The width of CNN kernels ??? CNN的kernel 到底是啥???\n",
    "#model = 'LSTMTDNN' # type of model to train and test, LSTM/LSTMTDNN\n",
    "# data_dir\n",
    "# dataset\n",
    "# checkpoint_dir\n",
    "forward_only = False # false for training\n",
    "use_char = True  # use 字-lv 语言模型\n",
    "use_word = False # use 词-lv 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 写入\n",
    "def save(fname, obj):\n",
    "    with open(fname, 'w') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# 读取\n",
    "def load(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fname = 'data/wxb_gt_train.txt' # 黄金时代-> train\n",
    "valid_fname = 'data/wxb_s_valid.txt'  # 沉默的大多数-> valid\n",
    "test_fname = 'data/wxb_sf_test.txt'   # 思维的乐趣-> test\n",
    "input_fnames = [train_fname, valid_fname, test_fname]\n",
    "\n",
    "# vocab_fname = 'vocab.pkl'\n",
    "# tensor_fname = 'data.pkl'\n",
    "# char_fname = 'data_char.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wxb_gt_train.txt  with characters  32443\n",
      "data/wxb_gt_train.txt with unique characters  1829\n",
      "data/wxb_s_valid.txt  with characters  9069\n",
      "data/wxb_s_valid.txt with unique characters  1059\n",
      "data/wxb_sf_test.txt  with characters  6215\n",
      "data/wxb_sf_test.txt with unique characters  867\n"
     ]
    }
   ],
   "source": [
    "# 处理 txt, 去掉空格空行\n",
    "counts = []\n",
    "for input_file in input_fnames:\n",
    "    with codecs.open(input_file,'rw','utf-8') as f: # 用 codecs 打开 file 才不会出现编码问题\n",
    "        a = f.read()\n",
    "        a = a.replace(' ','') # 去掉空格\n",
    "        a = a.replace('\\n','') # 去掉空行\n",
    "        print input_file,' with characters ',len(a) # 每个 txt 中有多少字\n",
    "        # print a[:20]\n",
    "        print input_file,'with unique characters ',len(set(a))\n",
    "        #print 'sample of characters: ',list(set(a))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用字: 一级常用字为3500, 二级常用字为3000字, 三级为姓氏人名地名等专有名词1605字, 总共8105字, http://www.gov.cn/gzdt/att/att/site1/20130819/tygfhzb.pdf\n",
    "\n",
    "使用 HSK 字典, 总字数为2500常用字\n",
    "https://github.com/OpenMindClub/OMAIWriter/wiki/ResDictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HSKc = pickle.load(codecs.open('data/HSKc.compatible.pickle', 'rb'))\n",
    "obj=[]\n",
    "for i in range(6):\n",
    "    f = codecs.open('data/hsk_chars_L'+str(i+1)+'.txt','rb','utf-8')\n",
    "    obj.append(f.read())\n",
    "\n",
    "pickle.dump(obj, open('data/hsk_char.pkl','w'),2) # write to a file, can reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HSKc = pickle.load(codecs.open('data/hsk_char.pkl','rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爱\n",
      "八\n",
      "爸\n",
      "杯\n",
      "本\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print HSKc[0][:10] # 有\\n 空行, 需要去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HSKc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = []\n",
    "for i in range(len(HSKc)):\n",
    "    t = HSKc[i].split('\\n')\n",
    "    chars.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars_all=[]\n",
    "for li in chars:\n",
    "    for c in li:\n",
    "        chars_all.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2504"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ooc = [] # out of character, 不在2500常用字典里\n",
    "for input_file in input_fnames:\n",
    "    with codecs.open(input_file,'rw','utf-8') as f:\n",
    "        a = f.read()\n",
    "        a = a.replace(' ','') \n",
    "        a = a.replace('\\n','')\n",
    "        a = list(set(a))\n",
    "        #print a[10]\n",
    "        for i in range(len(a)):\n",
    "            if a[i] not in chars_all:\n",
    "                ooc.append(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黝 蚯 罪 呸 》 酡 孽 芭 玷 甸 槛 裆 泞 薅 砂 坎 恬 秽 尿 撅 钥 薯 澳 汊 玻 秧 氓 泵 缕 粪 傣 窿 腮 裳 凹 妾 葫 喋 唠 墩 . 琶 卤 泱 蚂 瞌 耙 碾 凸 崽 蚁 粽 篓 蒂 爹 襟 瓢 寨 籽 箍 溯 蕉 、 匪 膛 蜴 屎 赘 彤 哩 1 赃 勒 叼 褂 “ 剐 寡 啪 睾 胳 乃 肝 笠 2 撵 Ｘ 豚 笋 吕 ” 筋 呐 苟 蹬 绷 匕 3 嚼 糠 潭 拴 沁 ， 爪 淫 奸 蝇 绢 咧 芳 4 腊 毙 袄 膊 l 垦 懊 弛 哼 ） 鸶 咒 棚 啧 叨 璃 蟥 蜥 蛤 浩 咙 烁 ？ ! 黏 a 坯 窑 籁 疚 耷 匠 犁 。 （ 皙 抡 炭 袒 腌 硝 藤 庇 骟 耸 赦 驴 捺 郭 镀 婊 搡 伧 9 y 刘 褪 莹 浒 哗 d 腺 刃 匙 倚 涧 唬 洼 矫 苹 ； 吭 掸 拎 ： 钳 莽 韵 缅 涎 窟 蹋 腕 脐 怦 撩 俏 靴 拽 蔓 戽 蜿 囊 哞 蚓 哟 疟 毋 杨 阉 掐 芦 跤 臼 唆 蜒 犟 铛 敦 跷 燎 揪 裸 妃 蓑 凳 藐 罗 鹭 — 哇 ！ 耿 《 铝 炊 擒 嗜 搀 箍 ： 贞 藉 癔 盥 拷 》 灶 懦 诲 暧 侏 髦 屎 柯 、 钎 1 ？ 啃 亢 儒 “ 喇 · 9 疙 叭 狐 w 。 （ 皙 宛 亩 佐 r 瘩 昧 迸 窟 芯 窿 u 募 嗡 捺 螂 s 艾 6 喋 ， 辄 奸 揪 t ” i 痰 — 冥 4 哇 腊 d ！ 莎 蒜 萧 崽 o 狸 蚁 《 ） 篓 吏 蒂 ； 吭 嗜 0 钳 7 罪 黯 扼 》 ： 盔 奸 艾 浩 迪 孽 、 邦 1 菲 儒 “ · 9 寡 檐 乃 莱 。 （ 垣 2 虞 ； 暇 ） ？ 屠 焉 丟 黜 孵 ， 淫 ” 爵 懈 兹 罗 赫 仲 — 4 尼 ！ 莎 萧 《 茨 徐 邪 凄 恕\n"
     ]
    }
   ],
   "source": [
    "for i in range(385):\n",
    "    print ooc[i],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察不在字典表中的字, 似乎有不少也是常见的字, 这个字典表2500字似乎还是略小....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现char-level 的[代码](https://github.com/carpedm20/lstm-char-cnn-tensorflow)中, 会将 txt 数据转化为 matrix时, 每一行为一个一个单词, 而 col 数目即为所有的字母, 对应中文应该怎样处理? col 数目假定为字典数目, 至少3000, 那每一行是什么? 也是词? 那么还是要分词吗? 还是取词典数量? 词典数量过大, 似乎还是分词靠谱一些? 不太理解啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
